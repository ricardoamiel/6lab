{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1 - PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obra',\n",
       " 'comienz',\n",
       " 'notici',\n",
       " 'celebraciã³n',\n",
       " 'cumpleaã±',\n",
       " 'bilb',\n",
       " 'bolsã³n',\n",
       " 'comarc',\n",
       " 'embarg',\n",
       " 'par']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Cargar el archivo de stoplist [CON ESTE TRABAJAREMOS countwords]\n",
    "with open('stoplist.txt', 'r') as file:\n",
    "    stoplist = file.read().splitlines()\n",
    "stoplist += ['.', ',', ';', ':', '!', '?', '¿', '¡', '(', ')', '[', ']', '{', '}', '\"', \"'\", '``', \"''\",\"111âº\"]\n",
    "\n",
    "lexema = SnowballStemmer('spanish')\n",
    "\n",
    "# Preprocesamiento\n",
    "def preprocesamiento(doc):\n",
    "    file = open(doc, \"r\")\n",
    "    # 1 - tokenizar\n",
    "    words = nltk.word_tokenize(file.read())\n",
    "    # 2 - normalizar\n",
    "    words = [word.lower() for word in words]\n",
    "    # 3 - reducción (stemming)\n",
    "    words = [lexema.stem(word) for word in words]\n",
    "    # 4 - eliminar stopwords\n",
    "    words = [word for word in words if word not in stoplist]\n",
    "    return words\n",
    "\n",
    "# Muestra de preprocesamiento\n",
    "preprocesamiento('libro1.txt')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2 - CONSTRUCCIÓN DE ÍNDICE INVERTIDO DE LOS 500 TÉRMINOS MÁS COMUNES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('abism', [(2, 1), (3, 2)]), ('acab', [(1, 3), (4, 2), (5, 1), (6, 2)]), ('acabã³', [(1, 1)])]\n"
     ]
    }
   ],
   "source": [
    "# Indice invertido\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "# saquemos las 500 palabras más frecuentes\n",
    "lista_libros= []\n",
    "lista_libros.append(preprocesamiento('libro1.txt'))\n",
    "lista_libros.append(preprocesamiento('libro2.txt'))\n",
    "lista_libros.append(preprocesamiento('libro3.txt'))\n",
    "lista_libros.append(preprocesamiento('libro4.txt'))\n",
    "lista_libros.append(preprocesamiento('libro5.txt'))\n",
    "lista_libros.append(preprocesamiento('libro6.txt'))\n",
    "\n",
    "word_freq = Counter()\n",
    "# Contar la frecuencia de cada palabra en cada libro preprocesado\n",
    "for text in lista_libros:\n",
    "    word_freq.update(text)\n",
    "    \n",
    "most_commom_words = word_freq.most_common(500)\n",
    "most_commom_words = [word for word, freq_word in most_commom_words]\n",
    "list_docs = ['libro1.txt','libro2.txt','libro3.txt','libro4.txt','libro5.txt','libro6.txt']\n",
    "\n",
    "# a) construir el índice invertido de las 500 palabras más frecuentes [lexemas]\n",
    "def indice_invertido_common_words(docs, most_common_words):\n",
    "    index = defaultdict(lambda: defaultdict(int))\n",
    "    for i, doc in enumerate(docs):\n",
    "        words = preprocesamiento(doc)\n",
    "        for word in words:\n",
    "            if word in most_common_words:\n",
    "                index[word][i+1] += 1\n",
    "\n",
    "    # Convertir el índice a listas ordenadas de tuplas\n",
    "    for word, doc_freqs in index.items():\n",
    "        index[word] = sorted(doc_freqs.items())\n",
    "        \n",
    "    index = sorted(index.items())\n",
    "    \n",
    "    # Convertir el índice a una cadena de texto\n",
    "    index_str = \"\"\n",
    "    for word, doc_freqs in index:\n",
    "        index_str += f\"{word},{len(doc_freqs)} -> \"\n",
    "        index_str += \" -> \".join(f\"{doc_id}\" for doc_id, _ in doc_freqs)\n",
    "        index_str += \"\\n\"\n",
    "\n",
    "    # Guardar el índice en un archivo de texto\n",
    "    with open(\"indice_invertido_500_FLECHA.txt\", \"w\") as file:\n",
    "        file.write(index_str)\n",
    "    \n",
    "    return index\n",
    "\n",
    "#Muestra de índice invertido de las 500 palabras más frecuentes\n",
    "print(indice_invertido_common_words(list_docs, most_commom_words)[0:3])\n",
    "        \n",
    "# b) guardar el índice en un archivo de texto[lexemas]\n",
    "def guardar_indice_invertido(docs, most_common_words):\n",
    "    index = indice_invertido_common_words(docs, most_common_words)\n",
    "    # Convertir el índice a una cadena de texto\n",
    "    index_str = \"\"\n",
    "    for word, doc_freqs in index:\n",
    "        index_str += f\"{word}: \"\n",
    "        index_str += \", \".join(f\"{doc_id}\" for doc_id, _ in doc_freqs)\n",
    "        index_str += \"\\n\"\n",
    "    with open('indice_invertido_500_COMAS.txt', 'w') as file:\n",
    "        file.write(index_str)\n",
    "        \n",
    "guardar_indice_invertido(list_docs,most_commom_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3 - APLICAR CONSULTAS BOOLEANAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comunidad en los libros:  [2]\n",
      "Frodo en los libros:  [1, 2, 3, 4, 5, 6]\n",
      "Gondor en los libros:  [2, 3, 5, 6]\n",
      "\n",
      "comunidad AND frodo: [2]\n",
      "comunidad OR frodo: [1, 2, 3, 4, 5, 6]\n",
      "NOT comunidad: [1, 3, 4, 5, 6]\n",
      "frodo AND-NOT gnndor: [1, 4]\n",
      "\n",
      "Ejemplo de consultas:\n",
      "\n",
      "(comunidad AND frodo) AND-NOT gondor: []\n",
      "(comunidad AND frodo) OR gondor: [2, 3, 5, 6]\n",
      "(gandalf AND-NOT hermana) OR gracias: [1, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# consultas booleanas\n",
    "\n",
    "list_libros = [1, 2, 3, 4, 5, 6] # libros del 1 al 6\n",
    "\n",
    "lexema = SnowballStemmer('spanish')\n",
    "\n",
    "def L(word): # busca en que libros se encuentra el lexema\n",
    "    result = []\n",
    "    for i, text in enumerate(lista_libros, start=1):\n",
    "        # si el lexema está en el texto (ej : lexema = comun => comunidad SI APARECE)\n",
    "        if lexema.stem(word) in text:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "def AND(A, B): # retorna los libros en los que se encuentran ambas palabras\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while i < len(A) and j < len(B):\n",
    "        if A[i] == B[j]:\n",
    "            result.append(A[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif A[i] < B[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return result # lo mismo que: [i for i in A if i in B]\n",
    "\n",
    "\n",
    "def OR(A, B): # retorna los libros en los que se encuentran al menos una de las palabras\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while i < len(A) and j < len(B):\n",
    "        if A[i] == B[j]:\n",
    "            result.append(A[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif A[i] < B[j]:\n",
    "            result.append(A[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(B[j])\n",
    "            j += 1\n",
    "    while i < len(A):\n",
    "        result.append(A[i])\n",
    "        i += 1\n",
    "    while j < len(B):\n",
    "        result.append(B[j])\n",
    "        j += 1\n",
    "    return result # lo mismo que A + list(set(B) - set(A)), pero en orden\n",
    "\n",
    "def NOT(A): # retorna los libros en los que no se encuentra la palabra\n",
    "    lista = [i for i in list_libros if i not in A]\n",
    "    return lista\n",
    "\n",
    "def ANDNOT(A,B): # retorna los libros en los que se encuentra la primera palabra pero no la segunda\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while i < len(A) and j < len(B):\n",
    "        if A[i] == B[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif A[i] < B[j]:\n",
    "            result.append(A[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    while i < len(A):\n",
    "        result.append(A[i])\n",
    "        i += 1\n",
    "    return result #lo mismo que [i for i in A if i not in B] pero en orden\n",
    "\n",
    "# Ejecución\n",
    "result1 = AND(L('comunidad'), L('frodo'))\n",
    "result2 = OR(L('comunidad'), L('frodo'))\n",
    "result3 = NOT(L('comunidad'))\n",
    "result4 = ANDNOT(L('frodo'), L('gondor'))\n",
    "print(\"Comunidad en los libros: \", L('comunidad'))\n",
    "print(\"Frodo en los libros: \", L('frodo'))\n",
    "print(\"Gondor en los libros: \", L('gondor'))\n",
    "print(\"\")\n",
    "print(f'comunidad AND frodo: {result1}') # INTERSECCION\n",
    "print(f'comunidad OR frodo: {result2}') # UNION\n",
    "print(f'NOT comunidad: {result3}') # NEGACION\n",
    "print(f'frodo AND-NOT gnndor: {result4}') # DIFERENCIA\n",
    "\n",
    "print()\n",
    "print('Ejemplo de consultas:\\n')\n",
    "\n",
    "#b) Probar el programa con al menos 3 consultas y al menos 3 términos\n",
    "consulta1 = ANDNOT(AND(L('comunidad'), L('frodo')), L('gondor')) # [2] - [2,3,5,6] = []\n",
    "print(f'(comunidad AND frodo) AND-NOT gondor: {consulta1}')\n",
    "\n",
    "consulta2 = OR(AND(L('comunidad'), L('frodo')), L('gondor')) # [2] + [2,3,5,6] = [2,3,5,6]\n",
    "print(f'(comunidad AND frodo) OR gondor: {consulta2}')\n",
    "\n",
    "#usar las palabras gandalf, hermana y gracias\n",
    "consulta3 = OR(ANDNOT(L('gandalf'), L('hermana')), L('gracias')) # ([1,2,3,5,6] - [5,6]) + [1,4,5] = [1,2,3] + [1,4,5] = [1,2,3,4,5]\n",
    "print(f'(gandalf AND-NOT hermana) OR gracias: {consulta3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
