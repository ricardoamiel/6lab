{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1 - PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 PRIMEROS LEXEMAS [Libro 1]:  ['obra', 'comienz', 'notici', 'celebraciã³n', '111âº', 'cumpleaã±', 'bilb', 'bolsã³n', 'comarc', 'bilb']\n",
      "10 PRIMERAS PALABRAS [Libro 1]:  ['obra', 'comienza', 'noticia', 'celebraciã³n', '111âº', 'cumpleaã±os', 'bilbo', 'bolsã³n', 'comarca', 'bilbo']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "# Cargar el archivo de stoplist [CON ESTE TRABAJAREMOS countwords]\n",
    "with open('stoplist.txt', 'r') as file:\n",
    "    stoplist = file.read().splitlines()\n",
    "stoplist += ['.', ',', ';', ':', '!', '?', '¿', '¡', '(', ')', '[', ']', '{', '}', '\"', \"'\"]\n",
    "\n",
    "def filter_stoplist(text):\n",
    "    return ' '.join([word.lower() for word in text.split() if word not in stoplist])\n",
    "\n",
    "# a) Filtrar los stop words usando el stoplist.txt\n",
    "for i in range(1, 7):\n",
    "    with open(f'libro{i}.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "        text = re.sub(r'[.,;:!?¿¡()\\[\\]{}\"\\'-]', '', text) # b) Retirar los signos innecesarios\n",
    "        text = text.lower() # convertir a minúsculas para el stemming\n",
    "        text = filter_stoplist(text)\n",
    "        \n",
    "        with open(f'libro{i}_filtered.txt', 'w') as file:\n",
    "            file.write(text)\n",
    "            \n",
    "# b) Reemplazar cada palabra por su raíz (stemming)\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "lexema = SnowballStemmer('spanish')\n",
    "\n",
    "for i in range(1, 7):\n",
    "    with open(f'libro{i}_filtered.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "        text = ' '.join([lexema.stem(word) for word in text.split()])\n",
    "        with open(f'libro{i}_stemmed.txt', 'w') as file:\n",
    "            file.write(text)\n",
    "            \n",
    "# guardar en listas las palabras de cada texto\n",
    "texts = []\n",
    "for i in range(1, 7):\n",
    "    with open(f'libro{i}_stemmed.txt', 'r') as file:\n",
    "        texts.append(file.read().split())\n",
    "\n",
    "print('10 PRIMEROS LEXEMAS [Libro 1]: ',texts[0][0:10]) \n",
    "\n",
    "texts2 = []\n",
    "for i in range(1, 7):\n",
    "    with open(f'libro{i}_filtered.txt', 'r') as file:\n",
    "        texts2.append(file.read().split())\n",
    "        \n",
    "print('10 PRIMERAS PALABRAS [Libro 1]: ',texts2[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2 - CONSTRUCCIÓN DE ÍNDICE INVERTIDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra de índice invertido:\n",
      " \n",
      "abismo: 2,1, 3,2\n",
      "acaba: 1,1, 4,2, 5,1\n",
      "acababa: 1,1\n",
      "acaban: 1,1, 6,1\n",
      "acabã³: 1,1\n",
      "accidentalmente: 1,1\n"
     ]
    }
   ],
   "source": [
    "# contar la frecuencia de cada lexema\n",
    "from collections import Counter\n",
    "\n",
    "# contar la frecuencia de cada palabra   \n",
    "word_freq = Counter()\n",
    "for text in texts2:\n",
    "    word_freq.update(text)\n",
    "    \n",
    "#print('FRECUENCIA DE PALABRAS: ',word_freq)\n",
    "        \n",
    "# seleccionar las 500 palabras más frecuentes\n",
    "most_commom_words = word_freq.most_common(500)\n",
    "most_commom_words = [word for word, freq_word in most_commom_words]\n",
    "\n",
    "# a) construir el índice invertido [palabras]\n",
    "index_words = {}\n",
    "for i, textword in enumerate(texts2, start=1):\n",
    "    word_freq = {}\n",
    "    for word1 in textword:\n",
    "        if word1 in most_commom_words:\n",
    "            word_freq[word1] = word_freq.get(word1, 0) + 1\n",
    "    for word1, freq_word in word_freq.items():\n",
    "        if word1 not in index_words:\n",
    "            index_words[word1] = []\n",
    "        index_words[word1].append((i, freq_word))\n",
    "            \n",
    "# b) guardar el índice en un archivo de texto[PALABRAS]\n",
    "with open('index_words.txt', 'w') as file:\n",
    "    for word1 in sorted(index_words.keys()): # ordenar el índice alfabéticamente\n",
    "        file.write(f'{word1}: {\", \".join([f\"{i},{j}\" for i, j in index_words[word1]])}\\n')\n",
    "        \n",
    "# leer el archivo de texto\n",
    "with open('index_words.txt', 'r') as file:\n",
    "    print(\"Muestra de índice invertido:\\n\",file.read()[10:111]) # mostrar entre 11 a 111 caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3 - APLICAR CONSULTAS BOOLEANAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comunidad en los libros:  [2]\n",
      "Frodo en los libros:  [1, 2, 3, 4, 5, 6]\n",
      "Gondor en los libros:  [2, 3, 5, 6]\n",
      "\n",
      "comunidad AND frodo: [2]\n",
      "comunidad OR frodo: [1, 2, 3, 4, 5, 6]\n",
      "NOT comunidad: [1, 3, 4, 5, 6]\n",
      "frodo AND-NOT gnndor: [1, 4]\n",
      "\n",
      "Ejemplo de consultas:\n",
      "\n",
      "(comunidad AND frodo) AND-NOT gondor: []\n",
      "(comunidad AND frodo) OR gondor: [2, 3, 5, 6]\n",
      "(gandalf AND-NOT hermana) OR gracias: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "list_libros = [1, 2, 3, 4, 5, 6] # libros del 1 al 6\n",
    "\n",
    "def L(word): # busca en que libros se encuentra la palabra\n",
    "    result = []\n",
    "    for i, text in enumerate(texts2, start=1):\n",
    "        if word in text:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "def AND(A, B): # retorna los libros en los que se encuentran ambas palabras\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while i < len(A) and j < len(B):\n",
    "        if A[i] == B[j]:\n",
    "            result.append(A[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif A[i] < B[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return result # lo mismo que: [i for i in A if i in B]\n",
    "\n",
    "\n",
    "def OR(A, B): # retorna los libros en los que se encuentran al menos una de las palabras\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while i < len(A) and j < len(B):\n",
    "        if A[i] == B[j]:\n",
    "            result.append(A[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif A[i] < B[j]:\n",
    "            result.append(A[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(B[j])\n",
    "            j += 1\n",
    "    while i < len(A):\n",
    "        result.append(A[i])\n",
    "        i += 1\n",
    "    while j < len(B):\n",
    "        result.append(B[j])\n",
    "        j += 1\n",
    "    return result # lo mismo que A + list(set(B) - set(A)), pero en orden\n",
    "\n",
    "def NOT(A): # retorna los libros en los que no se encuentra la palabra\n",
    "    lista = [i for i in list_libros if i not in A]\n",
    "    return lista\n",
    "\n",
    "def ANDNOT(A,B): # retorna los libros en los que se encuentra la primera palabra pero no la segunda\n",
    "    i, j = 0, 0\n",
    "    result = []\n",
    "    while i < len(A) and j < len(B):\n",
    "        if A[i] == B[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif A[i] < B[j]:\n",
    "            result.append(A[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    while i < len(A):\n",
    "        result.append(A[i])\n",
    "        i += 1\n",
    "    return result #lo mismo que [i for i in A if i not in B] pero en orden\n",
    "\n",
    "# Ejecución\n",
    "result1 = AND(L('comunidad'), L('frodo'))\n",
    "result2 = OR(L('comunidad'), L('frodo'))\n",
    "result3 = NOT(L('comunidad'))\n",
    "result4 = ANDNOT(L('frodo'), L('gondor'))\n",
    "print(\"Comunidad en los libros: \", L('comunidad'))\n",
    "print(\"Frodo en los libros: \", L('frodo'))\n",
    "print(\"Gondor en los libros: \", L('gondor'))\n",
    "print(\"\")\n",
    "print(f'comunidad AND frodo: {result1}') # INTERSECCION\n",
    "print(f'comunidad OR frodo: {result2}') # UNION\n",
    "print(f'NOT comunidad: {result3}') # NEGACION\n",
    "print(f'frodo AND-NOT gnndor: {result4}') # DIFERENCIA\n",
    "\n",
    "print()\n",
    "print('Ejemplo de consultas:\\n')\n",
    "\n",
    "#b) Probar el programa con al menos 3 consultas y al menos 3 términos\n",
    "consulta1 = ANDNOT(AND(L('comunidad'), L('frodo')), L('gondor')) # [2] - [2,3,5,6] = []\n",
    "print(f'(comunidad AND frodo) AND-NOT gondor: {consulta1}')\n",
    "\n",
    "consulta2 = OR(AND(L('comunidad'), L('frodo')), L('gondor')) # [2] + [2,3,5,6] = [2,3,5,6]\n",
    "print(f'(comunidad AND frodo) OR gondor: {consulta2}')\n",
    "\n",
    "#usar las palabras gandalf, hermana y gracias\n",
    "consulta3 = OR(ANDNOT(L('gandalf'), L('hermana')), L('gracias')) # ([1,2,3,5,6] - [5,6]) + [1,4,5] = [1,2,3] + [1,4,5] = [1,2,3,4,5]\n",
    "print(f'(gandalf AND-NOT hermana) OR gracias: {consulta3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
